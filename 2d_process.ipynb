{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2n5wojDYrNj",
        "outputId": "3cc2356b-6b5b-44b5-c63d-261eea869516"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2vUe06sYu22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc1a9629-fbd2-41bf-9928-9c605d56bc7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykJwATE7Yvi3",
        "outputId": "b5542b7f-63ee-4fd5-d4b9-bac30fad40bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Pix2Pix\n",
            "config.py  dataset.py    discriminator_model.py  gen.pth.tar   train.py\n",
            "\u001b[0m\u001b[01;34mdata\u001b[0m/      disc.pth.tar  generator_model.py      \u001b[01;34m__pycache__\u001b[0m/  utils.py\n"
          ]
        }
      ],
      "source": [
        "%cd '//content//drive//My Drive//Pix2Pix'\n",
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgkvlhRPfPm0",
        "outputId": "4c841011-cec0-41db-cc58-c5125dc823c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000 3\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "train_folder = \"//content//drive//My Drive//Pix2Pix//data//maps//1000dataset//\"\n",
        "val_folder = \"//content//drive//My Drive//Pix2Pix//data//maps//val//\"\n",
        "\n",
        "tfiles = len(os.listdir(train_folder))\n",
        "vfiles = len(os.listdir(val_folder))\n",
        "\n",
        "print(tfiles, vfiles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5Zi2K-EY8jv"
      },
      "outputs": [],
      "source": [
        "# !unzip \"/content/drive/My Drive/2DGAN.zip\" -d \"/content/drive/My Drive/2DGAN\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwdJCNcgGyxJ"
      },
      "outputs": [],
      "source": [
        "root_dir = '/content/drive/MyDrive/Pix2Pix'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPaUVG88geF6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from torch.utils import data\n",
        "from torch.autograd import Variable\n",
        "import torch\n",
        "\n",
        "sys.path.insert(0,root_dir)\n",
        "sys.path.append(root_dir +\"/src/\")\n",
        "\n",
        "\n",
        "import config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "An9ybTfaJOoT"
      },
      "source": [
        "utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8f2cI7vHYCu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "def save_some_examples(gen, val_loader, epoch, folder):\n",
        "\n",
        "    for i, (x,y) in enumerate(val_loader):\n",
        "\n",
        "        x, y = x.to(config.DEVICE), y.to(config.DEVICE)\n",
        "        gen.eval()\n",
        "        with torch.no_grad():\n",
        "            y_fake = gen(x)\n",
        "            y_fake = y_fake * 0.5 + 0.5  # remove normalization#\n",
        "            save_image(y_fake, folder + f\"/y_gen_{i}_{epoch}.png\")\n",
        "            save_image(x * 0.5 + 0.5, folder + f\"/input_{i}_{epoch}.png\")\n",
        "            if epoch == 1:\n",
        "                save_image(y * 0.5 + 0.5, folder + f\"/label_{i}_{epoch}.png\")\n",
        "        gen.train()\n",
        "\n",
        "        # Change the value of i to how many ever images you would like to view\n",
        "        if i == 3:\n",
        "          break\n",
        "\n",
        "\n",
        "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    checkpoint = {\n",
        "        \"state_dict\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "    }\n",
        "    torch.save(checkpoint, filename)\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint_file, model, optimizer, lr):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    checkpoint = torch.load(checkpoint_file, map_location=config.DEVICE)\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "\n",
        "    # If we don't do this then it will just have learning rate of old checkpoint\n",
        "    # and it will lead to many hours of debugging \\:\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group[\"lr\"] = lr\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW9mq6xaJS-m"
      },
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6ebriNPHgzc"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "import config\n",
        "\n",
        "class MapDataset(Dataset):\n",
        "    def __init__(self, root_dir):\n",
        "        self.root_dir = root_dir\n",
        "        self.list_files = os.listdir(self.root_dir)\n",
        "        print(self.list_files)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.list_files)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_file = self.list_files[index]\n",
        "        img_path = os.path.join(self.root_dir, img_file)\n",
        "        image = np.array(Image.open(img_path))\n",
        "        input_image = image[:, :600, :]\n",
        "        target_image = image[:, 600:, :]\n",
        "\n",
        "        augmentations = config.both_transform(image=input_image, image0=target_image)\n",
        "        input_image, target_image = augmentations[\"image\"], augmentations[\"image0\"]\n",
        "\n",
        "        input_image = config.transform_only_input(image=input_image)[\"image\"]\n",
        "        target_image = config.transform_only_mask(image=target_image)[\"image\"]\n",
        "\n",
        "        return input_image, target_image\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSBXomJDJVVm"
      },
      "source": [
        "generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrDTSTXUHj4r",
        "outputId": "2cc94070-74bb-4b34-e894-89402cbb7d0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3, 256, 256])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, down=True, act=\"relu\", use_dropout=False):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 4, 2, 1, bias=False, padding_mode=\"reflect\")\n",
        "            if down\n",
        "            else nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU() if act == \"relu\" else nn.LeakyReLU(0.2),\n",
        "        )\n",
        "\n",
        "        self.use_dropout = use_dropout\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return self.dropout(x) if self.use_dropout else x\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, in_channels=3, features=64):\n",
        "        super().__init__()\n",
        "        self.initial_down = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, features, 4, 2, 1, padding_mode=\"reflect\"),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "\n",
        "        self.down1 = Block(features, features*2, down=True, act=\"leaky\", use_dropout=False)\n",
        "        self.down2 = Block(features*2, features*4, down=True, act=\"leaky\", use_dropout=False)\n",
        "        self.down3 = Block(features*4, features*8, down=True, act=\"leaky\", use_dropout=False)\n",
        "        self.down4 = Block(features*8, features*8, down=True, act=\"leaky\", use_dropout=False)\n",
        "        self.down5 = Block(features*8, features*8, down=True, act=\"leaky\", use_dropout=False)\n",
        "        self.down6 = Block(features*8, features*8, down=True, act=\"leaky\", use_dropout=False)\n",
        "        self.bottleneck = nn.Sequential(\n",
        "            nn.Conv2d(features*8, features*8, 4, 2, 1, padding_mode=\"reflect\"), nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.up1 = Block(features*8, features*8, down=False, act=\"relu\", use_dropout=True)\n",
        "        self.up2 = Block(features*8*2, features*8, down=False, act=\"relu\", use_dropout=True)\n",
        "        self.up3 = Block(features*8*2, features*8, down=False, act=\"relu\", use_dropout=True)\n",
        "        self.up4 = Block(features*8*2, features*8, down=False, act=\"relu\", use_dropout=False)\n",
        "        self.up5 = Block(features*8*2, features*4, down=False, act=\"relu\", use_dropout=False)\n",
        "        self.up6 = Block(features*4*2, features*2, down=False, act=\"relu\", use_dropout=False)\n",
        "        self.up7 = Block(features*2*2, features, down=False, act=\"relu\", use_dropout=False)\n",
        "        self.final_up = nn.Sequential(\n",
        "            nn.ConvTranspose2d(features*2, in_channels, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        d1 = self.initial_down(x)\n",
        "        d2 = self.down1(d1)\n",
        "        d3 = self.down2(d2)\n",
        "        d4 = self.down3(d3)\n",
        "        d5 = self.down4(d4)\n",
        "        d6 = self.down5(d5)\n",
        "        d7 = self.down6(d6)\n",
        "        bottleneck = self.bottleneck(d7)\n",
        "        up1 = self.up1(bottleneck)\n",
        "        up2 = self.up2(torch.cat([up1, d7], 1))\n",
        "        up3 = self.up3(torch.cat([up2, d6], 1))\n",
        "        up4 = self.up4(torch.cat([up3, d5], 1))\n",
        "        up5 = self.up5(torch.cat([up4, d4], 1))\n",
        "        up6 = self.up6(torch.cat([up5, d3], 1))\n",
        "        up7 = self.up7(torch.cat([up6, d2], 1))\n",
        "        return self.final_up(torch.cat([up7, d1], 1))\n",
        "\n",
        "\n",
        "def test():\n",
        "    x = torch.randn((1, 3, 256, 256))\n",
        "    model = Generator(in_channels=3, features=64)\n",
        "    preds = model(x)\n",
        "    print(preds.shape)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kc7LiTrBJYDR"
      },
      "source": [
        "discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vy3PHrhHHsHW",
        "outputId": "d8ceee81-006c-41fa-9c5a-2dd42e33ac22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 26, 26])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CNNBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=2):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 4, stride, bias=False, padding_mode=\"reflect\"),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, in_channels=3, features=[64, 128, 256, 512]):\n",
        "        super().__init__()\n",
        "        self.initial = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels*2,\n",
        "                features[0],\n",
        "                kernel_size=4,\n",
        "                stride=2,\n",
        "                padding=1,\n",
        "                padding_mode=\"reflect\",\n",
        "            ),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "\n",
        "        layers = []\n",
        "        in_channels = features[0]\n",
        "        for feature in features[1:]:\n",
        "            layers.append(\n",
        "                CNNBlock(in_channels, feature, stride=1 if feature == features[-1] else 2),\n",
        "            )\n",
        "            in_channels = feature\n",
        "\n",
        "        layers.append(\n",
        "            nn.Conv2d(\n",
        "                in_channels, 1, kernel_size=4, stride=1, padding=1, padding_mode=\"reflect\"\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        x = torch.cat([x, y], dim=1)\n",
        "        x = self.initial(x)\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "\n",
        "def test():\n",
        "    x = torch.randn((1, 3, 256, 256))\n",
        "    y = torch.randn((1, 3, 256, 256))\n",
        "    model = Discriminator(in_channels=3)\n",
        "    preds = model(x, y)\n",
        "    print(preds.shape)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEomGAH0JbSa"
      },
      "source": [
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XpTMvCE3HDMb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import config\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "def train_fn(disc, gen, loader, opt_disc, opt_gen, l1, bce, g_scaler, d_scaler):\n",
        "    loop = tqdm(loader, leave=True)\n",
        "\n",
        "    for idx, (x, y) in enumerate(loop):\n",
        "        x, y = x.to(config.DEVICE), y.to(config.DEVICE)\n",
        "\n",
        "        # Train Discriminator\n",
        "        with torch.cuda.amp.autocast():\n",
        "            y_fake = gen(x)\n",
        "            D_real = disc(x, y)\n",
        "            D_fake = disc(x, y_fake.detach())\n",
        "            D_real_loss = bce(D_real, torch.ones_like(D_real))\n",
        "            D_fake_loss = bce(D_fake, torch.zeros_like(D_fake))\n",
        "            D_loss = (D_real_loss + D_fake_loss) / 2\n",
        "\n",
        "        disc.zero_grad()\n",
        "        d_scaler.scale(D_loss).backward()\n",
        "        d_scaler.step(opt_disc)\n",
        "        d_scaler.update()\n",
        "\n",
        "        # Train generator\n",
        "        with torch.cuda.amp.autocast():\n",
        "            D_fake = disc(x, y_fake)\n",
        "            G_fake_loss = bce(D_fake, torch.ones_like(D_fake))\n",
        "            L1 = l1(y_fake, y) * config.L1_LAMBDA\n",
        "            G_loss = G_fake_loss + L1\n",
        "\n",
        "        opt_gen.zero_grad()\n",
        "        g_scaler.scale(G_loss).backward()\n",
        "        g_scaler.step(opt_gen)\n",
        "        g_scaler.update()\n",
        "    print(D_loss, G_loss)\n",
        "    return D_loss, G_loss\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Px-TOpxtFFLh"
      },
      "source": [
        "# Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        },
        "id": "Icm2YYXJDqLN",
        "outputId": "18b2ccde-84fa-4a88-e63d-73d379a67a6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Loading checkpoint\n",
            "=> Loading checkpoint\n",
            "['43.jpg', '44.jpg', '48.jpg', '49.jpg', '50.jpg', '51.jpg', '52.jpg', '45.jpg', '46.jpg', '47.jpg', '53.jpg', '54.jpg', '55.jpg', '56.jpg', '61.jpg', '62.jpg', '63.jpg', '64.jpg', '65.jpg', '66.jpg', '67.jpg', '68.jpg', '69.jpg', '70.jpg', '71.jpg', '72.jpg', '73.jpg', '74.jpg', '75.jpg', '76.jpg', '34.jpg', '33.jpg', '36.jpg', '35.jpg', '29.jpg', '30.jpg', '32.jpg', '31.jpg', '25.jpg', '21.jpg', '17.jpg', '13.jpg', '9.jpg', '5.jpg', '1.jpg', '26.jpg', '27.jpg', '28.jpg', '22.jpg', '23.jpg', '24.jpg', '18.jpg', '19.jpg', '20.jpg', '14.jpg', '15.jpg', '16.jpg', '10.jpg', '11.jpg', '12.jpg', '6.jpg', '7.jpg', '8.jpg', '2.jpg', '3.jpg', '4.jpg', '81.jpg', '82.jpg', '83.jpg', '84.jpg', '200.jpg', '201.jpg', '202.jpg', '203.jpg', '204.jpg', '205.jpg', '206.jpg', '207.jpg', '208.jpg', '209.jpg', '210.jpg', '211.jpg', '212.jpg', '213.jpg', '214.jpg', '215.jpg', '216.jpg', '217.jpg', '218.jpg', '223.jpg', '222.jpg', '219.jpg', '220.jpg', '221.jpg', '224.jpg', '225.jpg', '226.jpg', '227.jpg', '228.jpg', '229.jpg', '230.jpg', '231.jpg', '232.jpg', '233.jpg', '234.jpg', '235.jpg', '236.jpg', '237.jpg', '238.jpg', '239.jpg', '240.jpg', '241.jpg', '242.jpg', '243.jpg', '244.jpg', '245.jpg', '246.jpg', '247.jpg', '248.jpg', '249.jpg', '250.jpg', '251.jpg', '85.jpg', '86.jpg', '87.jpg', '88.jpg', '89.jpg', '90.jpg', '91.jpg', '92.jpg', '93.jpg', '94.jpg', '95.jpg', '96.jpg', '41.jpg', '42.jpg', '97.jpg', '98.jpg', '99.jpg', '100.jpg', '101.jpg', '102.jpg', '103.jpg', '104.jpg', '105.jpg', '106.jpg', '107.jpg', '108.jpg', '109.jpg', '110.jpg', '111.jpg', '112.jpg', '113.jpg', '114.jpg', '115.jpg', '116.jpg', '117.jpg', '118.jpg', '119.jpg', '120.jpg', '121.jpg', '122.jpg', '123.jpg', '124.jpg', '125.jpg', '126.jpg', '127.jpg', '128.jpg', '129.jpg', '130.jpg', '131.jpg', '132.jpg', '133.jpg', '134.jpg', '135.jpg', '136.jpg', '137.jpg', '138.jpg', '139.jpg', '140.jpg', '141.jpg', '142.jpg', '143.jpg', '144.jpg', '145.jpg', '146.jpg', '147.jpg', '252.jpg', '253.jpg', '254.jpg', '255.jpg', '148.jpg', '149.jpg', '150.jpg', '151.jpg', '152.jpg', '153.jpg', '154.jpg', '155.jpg', '156.jpg', '157.jpg', '158.jpg', '159.jpg', '160.jpg', '161.jpg', '162.jpg', '163.jpg', '164.jpg', '165.jpg', '166.jpg', '167.jpg', '168.jpg', '169.jpg', '256.jpg', '170.jpg', '257.jpg', '171.jpg', '258.jpg', '172.jpg', '173.jpg', '259.jpg', '174.jpg', '175.jpg', '176.jpg', '177.jpg', '178.jpg', '179.jpg', '180.jpg', '181.jpg', '182.jpg', '183.jpg', '184.jpg', '185.jpg', '186.jpg', '187.jpg', '493.jpg', '494.jpg', '495.jpg', '496.jpg', '188.jpg', '189.jpg', '190.jpg', '191.jpg', '192.jpg', '193.jpg', '195.jpg', '194.jpg', '199.jpg', '196.jpg', '197.jpg', '198.jpg', '260.jpg', '261.jpg', '263.jpg', '264.jpg', '265.jpg', '266.jpg', '267.jpg', '268.jpg', '269.jpg', '270.jpg', '271.jpg', '272.jpg', '273.jpg', '274.jpg', '275.jpg', '276.jpg', '277.jpg', '278.jpg', '262.jpg', '279.jpg', '280.jpg', '281.jpg', '282.jpg', '283.jpg', '284.jpg', '285.jpg', '286.jpg', '287.jpg', '288.jpg', '289.jpg', '290.jpg', '291.jpg', '292.jpg', '293.jpg', '294.jpg', '295.jpg', '296.jpg', '297.jpg', '298.jpg', '299.jpg', '300.jpg', '301.jpg', '302.jpg', '303.jpg', '304.jpg', '305.jpg', '306.jpg', '307.jpg', '308.jpg', '309.jpg', '310.jpg', '311.jpg', '312.jpg', '313.jpg', '314.jpg', '315.jpg', '316.jpg', '317.jpg', '318.jpg', '319.jpg', '320.jpg', '321.jpg', '322.jpg', '323.jpg', '324.jpg', '325.jpg', '326.jpg', '327.jpg', '328.jpg', '329.jpg', '330.jpg', '331.jpg', '332.jpg', '333.jpg', '334.jpg', '335.jpg', '336.jpg', '337.jpg', '338.jpg', '339.jpg', '340.jpg', '341.jpg', '342.jpg', '343.jpg', '344.jpg', '345.jpg', '346.jpg', '347.jpg', '348.jpg', '349.jpg', '350.jpg', '351.jpg', '492.jpg', '491.jpg', '490.jpg', '489.jpg', '352.jpg', '353.jpg', '354.jpg', '355.jpg', '356.jpg', '357.jpg', '358.jpg', '359.jpg', '488.jpg', '487.jpg', '486.jpg', '485.jpg', '360.jpg', '361.jpg', '362.jpg', '363.jpg', '364.jpg', '365.jpg', '366.jpg', '367.jpg', '484.jpg', '483.jpg', '482.jpg', '481.jpg', '477.jpg', '480.jpg', '479.jpg', '478.jpg', '368.jpg', '369.jpg', '370.jpg', '371.jpg', '476.jpg', '475.jpg', '474.jpg', '473.jpg', '372.jpg', '373.jpg', '374.jpg', '375.jpg', '376.jpg', '377.jpg', '378.jpg', '379.jpg', '380.jpg', '381.jpg', '382.jpg', '383.jpg', '472.jpg', '471.jpg', '470.jpg', '469.jpg', '384.jpg', '385.jpg', '386.jpg', '387.jpg', '388.jpg', '389.jpg', '390.jpg', '391.jpg', '392.jpg', '393.jpg', '394.jpg', '395.jpg', '396.jpg', '397.jpg', '398.jpg', '399.jpg', '400.jpg', '401.jpg', '402.jpg', '403.jpg', '404.jpg', '405.jpg', '406.jpg', '407.jpg', '468.jpg', '467.jpg', '466.jpg', '465.jpg', '408.jpg', '409.jpg', '410.jpg', '411.jpg', '464.jpg', '463.jpg', '462.jpg', '461.jpg', '412.jpg', '413.jpg', '414.jpg', '415.jpg', '416.jpg', '417.jpg', '418.jpg', '419.jpg', '460.jpg', '459.jpg', '458.jpg', '457.jpg', '420.jpg', '421.jpg', '422.jpg', '423.jpg', '456.jpg', '455.jpg', '454.jpg', '453.jpg', '424.jpg', '425.jpg', '426.jpg', '427.jpg', '428.jpg', '429.jpg', '430.jpg', '431.jpg', '432.jpg', '433.jpg', '434.jpg', '435.jpg', '436.jpg', '437.jpg', '438.jpg', '439.jpg', '440.jpg', '441.jpg', '442.jpg', '443.jpg', '444.jpg', '445.jpg', '446.jpg', '447.jpg', '448.jpg', '449.jpg', '450.jpg', '452.jpg', '451.jpg', '500.jpg', '499.jpg', '498.jpg', '497.jpg', '37.jpg', '38.jpg', '39.jpg', '40.jpg', '57.jpg', '58.jpg', '59.jpg', '60.jpg', '77.jpg', '78.jpg', '79.jpg', '80.jpg', '1_flipped.jpg', '100_flipped.jpg', '139_flipped.jpg', '141_flipped.jpg', '188_flipped.jpg', '156_flipped.jpg', '15_flipped.jpg', '154_flipped.jpg', '146_flipped.jpg', '123_flipped.jpg', '116_flipped.jpg', '197_flipped.jpg', '168_flipped.jpg', '167_flipped.jpg', '137_flipped.jpg', '115_flipped.jpg', '102_flipped.jpg', '172_flipped.jpg', '13_flipped.jpg', '151_flipped.jpg', '152_flipped.jpg', '130_flipped.jpg', '117_flipped.jpg', '181_flipped.jpg', '132_flipped.jpg', '166_flipped.jpg', '103_flipped.jpg', '122_flipped.jpg', '149_flipped.jpg', '184_flipped.jpg', '193_flipped.jpg', '190_flipped.jpg', '18_flipped.jpg', '170_flipped.jpg', '164_flipped.jpg', '191_flipped.jpg', '175_flipped.jpg', '129_flipped.jpg', '176_flipped.jpg', '178_flipped.jpg', '104_flipped.jpg', '148_flipped.jpg', '171_flipped.jpg', '186_flipped.jpg', '14_flipped.jpg', '177_flipped.jpg', '194_flipped.jpg', '138_flipped.jpg', '144_flipped.jpg', '110_flipped.jpg', '105_flipped.jpg', '162_flipped.jpg', '19_flipped.jpg', '10_flipped.jpg', '147_flipped.jpg', '189_flipped.jpg', '153_flipped.jpg', '179_flipped.jpg', '111_flipped.jpg', '101_flipped.jpg', '161_flipped.jpg', '150_flipped.jpg', '140_flipped.jpg', '155_flipped.jpg', '135_flipped.jpg', '160_flipped.jpg', '180_flipped.jpg', '187_flipped.jpg', '119_flipped.jpg', '173_flipped.jpg', '108_flipped.jpg', '169_flipped.jpg', '109_flipped.jpg', '183_flipped.jpg', '112_flipped.jpg', '114_flipped.jpg', '17_flipped.jpg', '157_flipped.jpg', '131_flipped.jpg', '126_flipped.jpg', '195_flipped.jpg', '118_flipped.jpg', '120_flipped.jpg', '113_flipped.jpg', '192_flipped.jpg', '165_flipped.jpg', '133_flipped.jpg', '134_flipped.jpg', '163_flipped.jpg', '196_flipped.jpg', '174_flipped.jpg', '182_flipped.jpg', '125_flipped.jpg', '124_flipped.jpg', '16_flipped.jpg', '143_flipped.jpg', '106_flipped.jpg', '158_flipped.jpg', '185_flipped.jpg', '145_flipped.jpg', '11_flipped.jpg', '142_flipped.jpg', '121_flipped.jpg', '128_flipped.jpg', '127_flipped.jpg', '107_flipped.jpg', '159_flipped.jpg', '12_flipped.jpg', '136_flipped.jpg', '200_flipped.jpg', '238_flipped.jpg', '2_flipped.jpg', '237_flipped.jpg', '280_flipped.jpg', '22_flipped.jpg', '207_flipped.jpg', '264_flipped.jpg', '253_flipped.jpg', '209_flipped.jpg', '227_flipped.jpg', '233_flipped.jpg', '217_flipped.jpg', '203_flipped.jpg', '268_flipped.jpg', '206_flipped.jpg', '236_flipped.jpg', '248_flipped.jpg', '241_flipped.jpg', '223_flipped.jpg', '204_flipped.jpg', '271_flipped.jpg', '277_flipped.jpg', '276_flipped.jpg', '263_flipped.jpg', '257_flipped.jpg', '260_flipped.jpg', '256_flipped.jpg', '21_flipped.jpg', '274_flipped.jpg', '218_flipped.jpg', '224_flipped.jpg', '20_flipped.jpg', '269_flipped.jpg', '222_flipped.jpg', '202_flipped.jpg', '265_flipped.jpg', '220_flipped.jpg', '234_flipped.jpg', '231_flipped.jpg', '230_flipped.jpg', '247_flipped.jpg', '259_flipped.jpg', '267_flipped.jpg', '208_flipped.jpg', '198_flipped.jpg', '213_flipped.jpg', '251_flipped.jpg', '258_flipped.jpg', '252_flipped.jpg', '228_flipped.jpg', '266_flipped.jpg', '243_flipped.jpg', '242_flipped.jpg', '235_flipped.jpg', '275_flipped.jpg', '270_flipped.jpg', '278_flipped.jpg', '272_flipped.jpg', '229_flipped.jpg', '249_flipped.jpg', '216_flipped.jpg', '262_flipped.jpg', '239_flipped.jpg', '201_flipped.jpg', '221_flipped.jpg', '212_flipped.jpg', '245_flipped.jpg', '273_flipped.jpg', '254_flipped.jpg', '246_flipped.jpg', '24_flipped.jpg', '219_flipped.jpg', '25_flipped.jpg', '27_flipped.jpg', '23_flipped.jpg', '261_flipped.jpg', '205_flipped.jpg', '225_flipped.jpg', '28_flipped.jpg', '226_flipped.jpg', '26_flipped.jpg', '232_flipped.jpg', '210_flipped.jpg', '211_flipped.jpg', '215_flipped.jpg', '199_flipped.jpg', '255_flipped.jpg', '279_flipped.jpg', '250_flipped.jpg', '214_flipped.jpg', '244_flipped.jpg', '240_flipped.jpg', '308_flipped.jpg', '315_flipped.jpg', '367_flipped.jpg', '297_flipped.jpg', '322_flipped.jpg', '357_flipped.jpg', '309_flipped.jpg', '364_flipped.jpg', '285_flipped.jpg', '361_flipped.jpg', '341_flipped.jpg', '304_flipped.jpg', '362_flipped.jpg', '358_flipped.jpg', '366_flipped.jpg', '326_flipped.jpg', '29_flipped.jpg', '338_flipped.jpg', '373_flipped.jpg', '312_flipped.jpg', '288_flipped.jpg', '350_flipped.jpg', '329_flipped.jpg', '332_flipped.jpg', '301_flipped.jpg', '313_flipped.jpg', '342_flipped.jpg', '323_flipped.jpg', '344_flipped.jpg', '369_flipped.jpg', '335_flipped.jpg', '32_flipped.jpg', '372_flipped.jpg', '298_flipped.jpg', '331_flipped.jpg', '356_flipped.jpg', '318_flipped.jpg', '290_flipped.jpg', '336_flipped.jpg', '303_flipped.jpg', '370_flipped.jpg', '354_flipped.jpg', '346_flipped.jpg', '307_flipped.jpg', '284_flipped.jpg', '368_flipped.jpg', '351_flipped.jpg', '35_flipped.jpg', '363_flipped.jpg', '37_flipped.jpg', '339_flipped.jpg', '327_flipped.jpg', '337_flipped.jpg', '296_flipped.jpg', '355_flipped.jpg', '305_flipped.jpg', '319_flipped.jpg', '287_flipped.jpg', '334_flipped.jpg', '3_flipped.jpg', '289_flipped.jpg', '295_flipped.jpg', '353_flipped.jpg', '359_flipped.jpg', '306_flipped.jpg', '310_flipped.jpg', '300_flipped.jpg', '330_flipped.jpg', '286_flipped.jpg', '360_flipped.jpg', '282_flipped.jpg', '347_flipped.jpg', '324_flipped.jpg', '345_flipped.jpg', '33_flipped.jpg', '320_flipped.jpg', '349_flipped.jpg', '292_flipped.jpg', '340_flipped.jpg', '321_flipped.jpg', '31_flipped.jpg', '294_flipped.jpg', '374_flipped.jpg', '299_flipped.jpg', '30_flipped.jpg', '311_flipped.jpg', '314_flipped.jpg', '348_flipped.jpg', '34_flipped.jpg', '343_flipped.jpg', '371_flipped.jpg', '325_flipped.jpg', '36_flipped.jpg', '283_flipped.jpg', '293_flipped.jpg', '352_flipped.jpg', '281_flipped.jpg', '328_flipped.jpg', '291_flipped.jpg', '333_flipped.jpg', '302_flipped.jpg', '317_flipped.jpg', '316_flipped.jpg', '365_flipped.jpg', '387_flipped.jpg', '44_flipped.jpg', '461_flipped.jpg', '435_flipped.jpg', '462_flipped.jpg', '466_flipped.jpg', '431_flipped.jpg', '385_flipped.jpg', '416_flipped.jpg', '381_flipped.jpg', '41_flipped.jpg', '459_flipped.jpg', '414_flipped.jpg', '401_flipped.jpg', '443_flipped.jpg', '4_flipped.jpg', '399_flipped.jpg', '406_flipped.jpg', '407_flipped.jpg', '42_flipped.jpg', '440_flipped.jpg', '397_flipped.jpg', '383_flipped.jpg', '421_flipped.jpg', '458_flipped.jpg', '434_flipped.jpg', '389_flipped.jpg', '391_flipped.jpg', '423_flipped.jpg', '390_flipped.jpg', '418_flipped.jpg', '438_flipped.jpg', '448_flipped.jpg', '427_flipped.jpg', '446_flipped.jpg', '394_flipped.jpg', '424_flipped.jpg', '392_flipped.jpg', '415_flipped.jpg', '405_flipped.jpg', '40_flipped.jpg', '439_flipped.jpg', '426_flipped.jpg', '441_flipped.jpg', '436_flipped.jpg', '463_flipped.jpg', '376_flipped.jpg', '395_flipped.jpg', '420_flipped.jpg', '39_flipped.jpg', '464_flipped.jpg', '375_flipped.jpg', '411_flipped.jpg', '430_flipped.jpg', '437_flipped.jpg', '457_flipped.jpg', '451_flipped.jpg', '398_flipped.jpg', '417_flipped.jpg', '449_flipped.jpg', '445_flipped.jpg', '442_flipped.jpg', '378_flipped.jpg', '454_flipped.jpg', '410_flipped.jpg', '428_flipped.jpg', '402_flipped.jpg', '432_flipped.jpg', '455_flipped.jpg', '419_flipped.jpg', '456_flipped.jpg', '45_flipped.jpg', '450_flipped.jpg', '413_flipped.jpg', '412_flipped.jpg', '404_flipped.jpg', '377_flipped.jpg', '429_flipped.jpg', '408_flipped.jpg', '400_flipped.jpg', '388_flipped.jpg', '380_flipped.jpg', '382_flipped.jpg', '444_flipped.jpg', '38_flipped.jpg', '396_flipped.jpg', '433_flipped.jpg', '409_flipped.jpg', '452_flipped.jpg', '384_flipped.jpg', '43_flipped.jpg', '379_flipped.jpg', '460_flipped.jpg', '465_flipped.jpg', '425_flipped.jpg', '453_flipped.jpg', '422_flipped.jpg', '403_flipped.jpg', '393_flipped.jpg', '447_flipped.jpg', '386_flipped.jpg', '46_flipped.jpg', '72_flipped.jpg', '500_flipped.jpg', '496_flipped.jpg', '476_flipped.jpg', '66_flipped.jpg', '90_flipped.jpg', '8_flipped.jpg', '479_flipped.jpg', '64_flipped.jpg', '49_flipped.jpg', '58_flipped.jpg', '482_flipped.jpg', '473_flipped.jpg', '63_flipped.jpg', '484_flipped.jpg', '492_flipped.jpg', '88_flipped.jpg', '65_flipped.jpg', '491_flipped.jpg', '78_flipped.jpg', '76_flipped.jpg', '9_flipped.jpg', '483_flipped.jpg', '55_flipped.jpg', '83_flipped.jpg', '89_flipped.jpg', '47_flipped.jpg', '67_flipped.jpg', '498_flipped.jpg', '81_flipped.jpg', '71_flipped.jpg', '93_flipped.jpg', '82_flipped.jpg', '52_flipped.jpg', '493_flipped.jpg', '51_flipped.jpg', '499_flipped.jpg', '472_flipped.jpg', '54_flipped.jpg', '87_flipped.jpg', '75_flipped.jpg', '468_flipped.jpg', '68_flipped.jpg', '61_flipped.jpg', '478_flipped.jpg', '487_flipped.jpg', '59_flipped.jpg', '469_flipped.jpg', '74_flipped.jpg', '490_flipped.jpg', '53_flipped.jpg', '481_flipped.jpg', '470_flipped.jpg', '486_flipped.jpg', '497_flipped.jpg', '488_flipped.jpg', '85_flipped.jpg', '50_flipped.jpg', '92_flipped.jpg', '73_flipped.jpg', '495_flipped.jpg', '80_flipped.jpg', '480_flipped.jpg', '70_flipped.jpg', '91_flipped.jpg', '475_flipped.jpg', '95_flipped.jpg', '7_flipped.jpg', '94_flipped.jpg', '56_flipped.jpg', '474_flipped.jpg', '48_flipped.jpg', '494_flipped.jpg', '62_flipped.jpg', '57_flipped.jpg', '471_flipped.jpg', '69_flipped.jpg', '5_flipped.jpg', '477_flipped.jpg', '79_flipped.jpg', '485_flipped.jpg', '60_flipped.jpg', '86_flipped.jpg', '6_flipped.jpg', '84_flipped.jpg', '467_flipped.jpg', '77_flipped.jpg', '489_flipped.jpg', '96_flipped.jpg', '98_flipped.jpg', '99_flipped.jpg', '97_flipped.jpg']\n",
            "['57.jpg', '37.jpg', '79.jpg']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 63/63 [00:42<00:00,  1.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.1502, device='cuda:0', grad_fn=<DivBackward0>) tensor(9.9686, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 14%|█▍        | 9/63 [00:03<00:19,  2.79it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-08be0176b01c>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mD_LOSS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG_LOSS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_disc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL1_LOSS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBCE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_scaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_scaler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVE_MODEL\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-85a60673d871>\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(disc, gen, loader, opt_disc, opt_gen, l1, bce, g_scaler, d_scaler)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mopt_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mg_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mg_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mg_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/cuda/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"No inf checks were recorded for this optimizer.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPPED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/cuda/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/cuda/amp/grad_scaler.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "disc = Discriminator(in_channels=3).to(config.DEVICE)\n",
        "gen = Generator(in_channels=3).to(config.DEVICE)\n",
        "opt_disc = optim.Adam(disc.parameters(), lr=config.LEARNING_RATE, betas=(0.5, 0.999))\n",
        "opt_gen = optim.Adam(gen.parameters(), lr=config.LEARNING_RATE, betas=(0.5, 0.999))\n",
        "BCE = nn.BCEWithLogitsLoss()\n",
        "L1_LOSS = nn.L1Loss()\n",
        "\n",
        "D_LOSS_history = []\n",
        "G_LOSS_history = []\n",
        "\n",
        "if config.LOAD_MODEL:\n",
        "    load_checkpoint(config.CHECKPOINT_GEN, gen, opt_gen, config.LEARNING_RATE)\n",
        "    load_checkpoint(config.CHECKPOINT_DISC, disc, opt_disc, config.LEARNING_RATE)\n",
        "\n",
        "train_dataset = MapDataset(root_dir=\"//content//drive//My Drive//Pix2Pix//data//maps//1000dataset\")\n",
        "train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=config.NUM_WORKERS)\n",
        "g_scaler = torch.cuda.amp.GradScaler()\n",
        "d_scaler = torch.cuda.amp.GradScaler()\n",
        "val_dataset = MapDataset(root_dir=\"//content//drive//My Drive//Pix2Pix//data//maps//val\")\n",
        "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "\n",
        "for epoch in range(config.NUM_EPOCHS):\n",
        "    D_LOSS, G_LOSS = train_fn(disc, gen, train_loader, opt_disc, opt_gen, L1_LOSS, BCE, g_scaler, d_scaler)\n",
        "\n",
        "    if config.SAVE_MODEL and epoch % 5 == 0:\n",
        "        save_checkpoint(gen, opt_gen, filename=config.CHECKPOINT_GEN)\n",
        "        save_checkpoint(disc, opt_disc, filename=config.CHECKPOINT_DISC)\n",
        "\n",
        "    save_some_examples(gen, val_loader, epoch, folder=\"//content//drive//My Drive//Pix2Pix//data//maps//evaluation\")\n",
        "\n",
        "    D_LOSS_history.append(D_LOSS.cpu().detach().numpy())\n",
        "    G_LOSS_history.append(G_LOSS.cpu().detach().numpy())\n",
        "\n",
        "plt.plot(np.arange(config.NUM_EPOCHS), D_LOSS_history, 'bo')\n",
        "plt.plot(np.arange(config.NUM_EPOCHS), G_LOSS_history, 'ro')\n",
        "\n",
        "plt.legend(['D_loss','G_loss'], loc='upper right')\n",
        "\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcSpNUyAE605"
      },
      "source": [
        "## Test here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Od3j3ovaE9e0",
        "outputId": "2063b075-69d7-4af3-93aa-e01d7a80fd72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Loading checkpoint\n",
            "=> Loading checkpoint\n",
            "['6.jpg', '5.jpg', '4.jpg', '2.jpg', '3.jpg', '1.jpg']\n"
          ]
        }
      ],
      "source": [
        "class TestDataset(Dataset):\n",
        "    def __init__(self, root_dir):\n",
        "        self.root_dir = root_dir\n",
        "        self.list_files = os.listdir(self.root_dir)\n",
        "        print(self.list_files)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.list_files)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_file = self.list_files[index]\n",
        "        img_path = os.path.join(self.root_dir, img_file)\n",
        "        image = np.array(Image.open(img_path))\n",
        "        input_image = image[:, :600, :]\n",
        "\n",
        "        augmentations = config.both_transform(image=input_image)\n",
        "        input_image = augmentations[\"image\"]\n",
        "\n",
        "        input_image = config.transform_only_input(image=input_image)[\"image\"]\n",
        "\n",
        "        return input_image\n",
        "\n",
        "\n",
        "def save_test_examples(gen, test_loader, epoch, folder):\n",
        "\n",
        "    for i, x in enumerate(test_loader):\n",
        "\n",
        "        x = x.to(config.DEVICE)\n",
        "        gen.eval()\n",
        "        with torch.no_grad():\n",
        "            y_fake = gen(x)\n",
        "            y_fake = y_fake * 0.5 + 0.5  # remove normalization#\n",
        "            save_image(y_fake, folder + f\"/y_gen_{i}_{epoch}.png\")\n",
        "            save_image(x * 0.5 + 0.5, folder + f\"/input_{i}_{epoch}.png\")\n",
        "\n",
        "        gen.train()\n",
        "\n",
        "\n",
        "disc = Discriminator(in_channels=3).to(config.DEVICE)\n",
        "gen = Generator(in_channels=3).to(config.DEVICE)\n",
        "\n",
        "if config.LOAD_MODEL:\n",
        "    load_checkpoint(config.CHECKPOINT_GEN, gen, opt_gen, config.LEARNING_RATE)\n",
        "    load_checkpoint(config.CHECKPOINT_DISC, disc, opt_disc, config.LEARNING_RATE)\n",
        "\n",
        "test_dataset = TestDataset(root_dir=\"//content//drive//My Drive//Pix2Pix//data//maps//test\")\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# Test model now\n",
        "save_test_examples(gen, test_loader, epoch, folder=\"//content//drive//My Drive//Pix2Pix//data//maps//testresult\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "in2sYnn0GDcv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}